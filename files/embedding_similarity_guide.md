# ğŸ“Š ì„ë² ë”© ëª¨ë¸ ë° ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ ê°€ì´ë“œ

> **ëª©ì **: RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì„ë² ë”© ëª¨ë¸ê³¼ ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ ì •ë¦¬  
> **ëŒ€ìƒ ë°ì´í„°**: Magazine Layout JSON (mood, category, description ë“± ì§§ì€ êµ¬ì¡°í™” í…ìŠ¤íŠ¸)

---

## ğŸ“‹ ìš”ì•½ ë¹„êµí‘œ

| ëª¨ë¸ | ì°¨ì› | ë¹„ìš© | í•œêµ­ì–´ | ì¶”ì²œ ìœ ì‚¬ë„ | íŠ¹ì§• |
|:---|:---:|:---:|:---:|:---|:---|
| **BGE-M3** | 1024 | ë¬´ë£Œ | â­• | Hybrid (Dense+Sparse) | í˜„ì¬ ì‚¬ìš© ì¤‘, ë‹¤êµ­ì–´ ì§€ì› |
| **Voyage-3** | 1024 | ìœ ë£Œ | â­• | Cosine | Fashion/E-commerce íŠ¹í™” |
| **Cohere embed-v3** | 1024 | ìœ ë£Œ | â­• | Input-type Aware | Query/Doc ë¶„ë¦¬ ì„ë² ë”© |
| **Jina v3** | 1024 | ë¬´ë£Œ | â­• | MaxSim (ColBERT) | í† í° ë‹¨ìœ„ Late-interaction |
| **OpenAI embed-3-large** | 3072 | ìœ ë£Œ | â­• | Cosine + MRL | ì°¨ì› ì¶•ì†Œ ê°€ëŠ¥ (Matryoshka) |

---

## 1ï¸âƒ£ BGE-M3 (í˜„ì¬ ì‚¬ìš© ì¤‘)

### ëª¨ë¸ ì •ë³´
- **ëª¨ë¸ëª…**: `BAAI/bge-m3`
- **ì°¨ì›**: 1024
- **ë¹„ìš©**: ë¬´ë£Œ (ë¡œì»¬ ì‹¤í–‰)
- **íŠ¹ì§•**: Dense + Sparse + ColBERT ë™ì‹œ ì§€ì›

### ì„ë² ë”© ì½”ë“œ
```python
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)

# Dense + Sparse ë™ì‹œ ì¶”ì¶œ
output = model.encode(
    ["Luxurious fashion editorial with serif fonts"],
    return_dense=True,
    return_sparse=True,
    return_colbert_vecs=False
)

dense_embedding = output['dense_vecs'][0]      # shape: (1024,)
sparse_weights = output['lexical_weights'][0]  # dict: {token_id: weight}
```

### ìœ ì‚¬ë„ ì¸¡ì •
```python
# Dense: Cosine Similarity
from numpy import dot
from numpy.linalg import norm

def cosine_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))

# Sparse: Lexical Matching Score
sparse_score = model.compute_lexical_matching_score(doc_sparse, query_sparse)

# Hybrid: RRF Fusion
def compute_rrf(dense_results, sparse_results, k=60):
    scores = defaultdict(float)
    for rank, doc_id in enumerate(dense_results):
        scores[doc_id] += 1 / (k + rank + 1)
    for rank, doc_id in enumerate(sparse_results):
        scores[doc_id] += 1 / (k + rank + 1)
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)
```

---

## 2ï¸âƒ£ Voyage AI voyage-3

### ëª¨ë¸ ì •ë³´
- **ëª¨ë¸ëª…**: `voyage-3`
- **ì°¨ì›**: 1024
- **ë¹„ìš©**: $0.06 / 1M tokens
- **íŠ¹ì§•**: Fashion, E-commerce ë„ë©”ì¸ MTEB 1ìœ„

### ì„ë² ë”© ì½”ë“œ
```python
import voyageai

client = voyageai.Client(api_key="your-api-key")

# ë¬¸ì„œ ì„ë² ë”©
doc_result = client.embed(
    ["Luxurious fashion editorial with serif fonts"],
    model="voyage-3",
    input_type="document"
)
doc_embedding = doc_result.embeddings[0]  # shape: (1024,)

# ì¿¼ë¦¬ ì„ë² ë”©
query_result = client.embed(
    ["minimalist beauty layout"],
    model="voyage-3",
    input_type="query"
)
query_embedding = query_result.embeddings[0]
```

### ìœ ì‚¬ë„ ì¸¡ì •
```python
# Cosine Similarity (ê¸°ë³¸)
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

similarity = cosine_similarity(query_embedding, doc_embedding)

# ë˜ëŠ” Voyage API ë‚´ì¥ ìœ ì‚¬ë„
# (ë³„ë„ ê³„ì‚° ë¶ˆí•„ìš”, APIì—ì„œ ì •ë ¬ëœ ê²°ê³¼ ë°˜í™˜ ê°€ëŠ¥)
```

---

## 3ï¸âƒ£ Cohere embed-v3

### ëª¨ë¸ ì •ë³´
- **ëª¨ë¸ëª…**: `embed-multilingual-v3.0`
- **ì°¨ì›**: 1024
- **ë¹„ìš©**: ìœ ë£Œ (ë¬´ë£Œ í‹°ì–´ ìˆìŒ)
- **íŠ¹ì§•**: input_typeìœ¼ë¡œ Document/Query êµ¬ë¶„ â†’ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ

### ì„ë² ë”© ì½”ë“œ
```python
import cohere

co = cohere.Client("your-api-key")

# ë¬¸ì„œ ì„ë² ë”© (indexingìš©)
doc_response = co.embed(
    texts=["Luxurious fashion editorial with serif fonts"],
    model="embed-multilingual-v3.0",
    input_type="search_document",  # â­ ë¬¸ì„œìš©
    embedding_types=["float"]
)
doc_embedding = doc_response.embeddings.float[0]

# ì¿¼ë¦¬ ì„ë² ë”© (ê²€ìƒ‰ìš©)
query_response = co.embed(
    texts=["minimalist beauty layout"],
    model="embed-multilingual-v3.0",
    input_type="search_query",  # â­ ì¿¼ë¦¬ìš©
    embedding_types=["float"]
)
query_embedding = query_response.embeddings.float[0]
```

### ìœ ì‚¬ë„ ì¸¡ì •
```python
# Dot Product (Cohere ê¶Œì¥)
import numpy as np

def dot_product(a, b):
    return np.dot(a, b)

similarity = dot_product(query_embedding, doc_embedding)

# ì°¸ê³ : CohereëŠ” ì •ê·œí™”ëœ ë²¡í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ
# Dot Product â‰ˆ Cosine Similarity
```

---

## 4ï¸âƒ£ Jina jina-embeddings-v3

### ëª¨ë¸ ì •ë³´
- **ëª¨ë¸ëª…**: `jinaai/jina-embeddings-v3`
- **ì°¨ì›**: 1024 (ë˜ëŠ” Late-interaction ì‹œ í† í°ë³„ ë²¡í„°)
- **ë¹„ìš©**: ë¬´ë£Œ (ë¡œì»¬) / API ìœ ë£Œ
- **íŠ¹ì§•**: ColBERT-style Late-interaction ì§€ì›

### ì„ë² ë”© ì½”ë“œ
```python
from transformers import AutoModel, AutoTokenizer
import torch

model = AutoModel.from_pretrained("jinaai/jina-embeddings-v3", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("jinaai/jina-embeddings-v3")

# ë‹¨ì¼ ë²¡í„° (Mean Pooling)
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # Mean pooling
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings[0].numpy()

doc_embedding = get_embedding("Luxurious fashion editorial")

# Late-Interaction (í† í°ë³„ ë²¡í„°)
def get_token_embeddings(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[0].numpy()  # shape: (seq_len, 1024)

doc_tokens = get_token_embeddings("Luxurious fashion editorial")
query_tokens = get_token_embeddings("luxury style")
```

### ìœ ì‚¬ë„ ì¸¡ì •: MaxSim (ColBERT-style)
```python
import numpy as np

def maxsim_score(query_tokens, doc_tokens):
    """
    ê° ì¿¼ë¦¬ í† í°ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ í† í°ì˜ ìœ ì‚¬ë„ë¥¼ í•©ì‚°
    """
    # query_tokens: (q_len, dim), doc_tokens: (d_len, dim)
    similarity_matrix = np.dot(query_tokens, doc_tokens.T)  # (q_len, d_len)
    
    # ê° ì¿¼ë¦¬ í† í°ì˜ ìµœëŒ€ ìœ ì‚¬ë„
    max_similarities = similarity_matrix.max(axis=1)  # (q_len,)
    
    return max_similarities.sum()

score = maxsim_score(query_tokens, doc_tokens)
```

---

## 5ï¸âƒ£ OpenAI text-embedding-3-large

### ëª¨ë¸ ì •ë³´
- **ëª¨ë¸ëª…**: `text-embedding-3-large`
- **ì°¨ì›**: 3072 (ê¸°ë³¸) â†’ **512, 1024 ë“±ìœ¼ë¡œ ì¶•ì†Œ ê°€ëŠ¥**
- **ë¹„ìš©**: $0.13 / 1M tokens
- **íŠ¹ì§•**: Matryoshka Representation Learning (MRL) - ì•ìª½ ì°¨ì›ë§Œ ì‚¬ìš©í•´ë„ ì„±ëŠ¥ ìœ ì§€

### ì„ë² ë”© ì½”ë“œ
```python
from openai import OpenAI

client = OpenAI(api_key="your-api-key")

# ê¸°ë³¸ 3072 ì°¨ì›
response = client.embeddings.create(
    model="text-embedding-3-large",
    input=["Luxurious fashion editorial with serif fonts"]
)
full_embedding = response.data[0].embedding  # len: 3072

# â­ ì°¨ì› ì¶•ì†Œ (Matryoshka)
response_512 = client.embeddings.create(
    model="text-embedding-3-large",
    input=["Luxurious fashion editorial with serif fonts"],
    dimensions=512  # 512, 1024, 1536 ë“± ì„ íƒ ê°€ëŠ¥
)
small_embedding = response_512.data[0].embedding  # len: 512
```

### ìœ ì‚¬ë„ ì¸¡ì •
```python
# Cosine Similarity
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# OpenAIëŠ” ì •ê·œí™”ëœ ë²¡í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ Dot Productë„ ë™ì¼
similarity = cosine_similarity(query_embedding, doc_embedding)
```

---

## ğŸ“ ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ ë¹„êµ

| ë°©ì‹ | ìˆ˜ì‹ | íŠ¹ì§• | ì¶”ì²œ ëª¨ë¸ |
|:---|:---|:---|:---|
| **Cosine Similarity** | `dot(a,b) / (â€–aâ€– Ã— â€–bâ€–)` | ë°©í–¥ ê¸°ë°˜, í¬ê¸° ë¬´ì‹œ | ëª¨ë“  ëª¨ë¸ |
| **Dot Product** | `dot(a,b)` | í¬ê¸°ë„ ë°˜ì˜, ì •ê·œí™”ëœ ë²¡í„°ë©´ Cosineê³¼ ë™ì¼ | Cohere, OpenAI |
| **MaxSim (ColBERT)** | `Î£ max(q_i Â· D)` | í† í° ë‹¨ìœ„ ë§¤ì¹­, ê¸´ ë¬¸ì„œì— ìœ ë¦¬ | Jina, BGE-M3 |
| **RRF Fusion** | `1/(k+rank)` | Dense+Sparse ê²°í•© | BGE-M3 (Hybrid) |

---

## ğŸ¯ ìš°ë¦¬ ë°ì´í„°ì…‹ì— ì¶”ì²œ

| ìš°ì„ ìˆœìœ„ | ì¡°í•© | ì´ìœ  |
|:---:|:---|:---|
| 1 | **BGE-M3 + Dense Only** | ë¬´ë£Œ, í˜„ì¬ ì½”ë“œ ìµœì†Œ ìˆ˜ì •, ì§§ì€ í…ìŠ¤íŠ¸ì— ì í•© |
| 2 | **Voyage-3 + Cosine** | Fashion/Beauty ë„ë©”ì¸ íŠ¹í™”, ë†’ì€ ì •í™•ë„ |
| 3 | **OpenAI + 512ì°¨ì›** | ì €ì¥ ë¹„ìš© ì ˆê°, ì•ˆì •ì ì¸ í’ˆì§ˆ |

---

*ë¬¸ì„œ ìƒì„±: 2026-01-15*
